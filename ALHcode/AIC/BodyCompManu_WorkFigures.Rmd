---
title: "BodyCompManu_WorkFigures"
author: "Abby Host"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### GSI Early Migratory Work vs condition Figures
```{r, include = FALSE}
library(dplyr)
library(ggplot2)

rm(list = ls())

#Download Work Data set
WorkData <- read.csv("lowerriver_GSI_PCscores.csv")[-1]
str(WorkData)
WorkData$Group_Assignment <- as.factor(WorkData$Group_Assignment)
levels(WorkData$Group_Assignment) #levels are not all consistent. Need to fix this

# Strip whitespace
WorkData$Group_Assignment <- trimws(WorkData$Group_Assignment)

# Standardize levels
WorkData$Group_Assignment <- factor(WorkData$Group_Assignment)
levels(WorkData$Group_Assignment)

# Clean up levels manually
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "Gulkana "] <- "Gulkana"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "Gulkanahatchery"] <- "GulkanaHatchery"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "KlutinaTonsinaOutlets"] <- "KlutinaTonsinaOutlet"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "insufficient genotypes"] <- "insufficient_genotypes"

# Then drop the insufficient genotypes
WorkData <- subset(WorkData, Group_Assignment != "insufficient_genotypes")
WorkData <- subset(WorkData, Group_Assignment != "LowerGroups")
WorkData$Group_Assignment <- droplevels(WorkData$Group_Assignment)



#download coefficient datasets for body size, energy density, total energy
body <- read.csv("bodysize_GSI/bodysize_GSI_ModelAvg_Coefs_withLikelihoods.csv", row.names = 1)
energy_mJkg <- read.csv("energydensity_GSI/EnergyPDry_1_GSI_ModelAvg_Coefs_withLikelihoods.csv", row.names = 1)
energy_mJ <- read.csv("totalenergy2_GSI/TotalEnergy_GSI_ModelAvg_Coefs_withLikelihoods.csv", row.names = 1)

#================ body size estimate calculations according to intercept =============
# Step 1: Extract intercept
str(body)
intercept_body <- body["(Intercept)", "Avg_Coef"]

# Step 2: Extract Group_Assignment effects
GSI_body <- body[grep("^Group_Assignment", rownames(body)), ]
GSI_body <- GSI_body[!grepl(":", rownames(GSI_body)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
GSI_body$GSI <- gsub("Group_Assignment", "", rownames(GSI_body))

# Step 3: Add intercept to each to get estimated body size
GSI_body$Estimated_BodySize <- intercept_body + GSI_body$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Group_Assignment)[1]  # Assumes first level is the reference
ref_row <- data.frame(
  Avg_Coef = intercept_body,
  CI_Lower = body["(Intercept)", "CI_Lower"],
  CI_Upper = body["(Intercept)", "CI_Upper"],
  SE = body["(Intercept)", "SE"],
  ParamLikelihood = body["(Intercept)", "ParamLikelihood"],
  GSI = ref_location,
  Estimated_BodySize = intercept_body
)

# Step 5: Combine
GSI_body <- rbind(ref_row, GSI_body)
rownames(GSI_body) <- NULL
GSI_body <- GSI_body[, c("GSI", setdiff(names(GSI_body), "GSI"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Group_Assignment)
avg_work_body <- WorkData %>%
  dplyr::group_by(Group_Assignment) %>%
  dplyr::summarise(Avg_Work = mean(Work_km2, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
GSI_body <- merge(GSI_body, avg_work_body, by.x = "GSI", by.y = "Group_Assignment") 
str(GSI_body) #final body size dataframe for work regressions





#================ energy density estimate calculations according to intercept =============
# Step 1: Extract intercept
str(energy_mJkg)
intercept_ED <- energy_mJkg["(Intercept)", "Avg_Coef"]

# Step 2: Extract Group_Assignment effects
GSI_ED <- energy_mJkg[grep("^Group_Assignment", rownames(energy_mJkg)), ]
GSI_ED <- GSI_ED[!grepl(":", rownames(GSI_ED)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
GSI_ED$GSI <- gsub("Group_Assignment", "", rownames(GSI_ED))

# Step 3: Add intercept to each to get estimated energy density
GSI_ED$Estimated_EnergyDensity <- intercept_ED + GSI_ED$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Group_Assignment)[1]  # Assumes first level is the reference
ref_row2 <- data.frame(
  Avg_Coef = intercept_ED,
  CI_Lower = energy_mJkg["(Intercept)", "CI_Lower"],
  CI_Upper = energy_mJkg["(Intercept)", "CI_Upper"],
  SE = energy_mJkg["(Intercept)", "SE"],
  ParamLikelihood = energy_mJkg["(Intercept)", "ParamLikelihood"],
  GSI = ref_location,
  Estimated_EnergyDensity = intercept_ED
)

# Step 5: Combine
GSI_ED <- rbind(ref_row2, GSI_ED)
rownames(GSI_ED) <- NULL
GSI_ED <- GSI_ED[, c("GSI", setdiff(names(GSI_ED), "GSI"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Group_Assignment)
avg_work_ED <- WorkData %>%
  dplyr::group_by(Group_Assignment) %>%
  dplyr::summarise(Avg_Work = mean(Work_km2, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
GSI_ED <- merge(GSI_ED, avg_work_ED, by.x = "GSI", by.y = "Group_Assignment") 
str(GSI_ED) #final energy density dataframe for work regressions



#================ total energy estimate calculations according to intercept =============
# Step 1: Extract intercept
str(energy_mJ)
intercept_TE <- energy_mJ["(Intercept)", "Avg_Coef"]

# Step 2: Extract Group_Assignment effects
GSI_TE <- energy_mJ[grep("^Group_Assignment", rownames(energy_mJ)), ]
GSI_TE <- GSI_TE[!grepl(":", rownames(GSI_TE)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
GSI_TE$GSI <- gsub("Group_Assignment", "", rownames(GSI_TE))

# Step 3: Add intercept to each to get estimated energy density
GSI_TE$Estimated_TotalEnergy <- intercept_TE + GSI_TE$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Group_Assignment)[1]  # Assumes first level is the reference
ref_row3 <- data.frame(
  Avg_Coef = intercept_TE,
  CI_Lower = energy_mJ["(Intercept)", "CI_Lower"],
  CI_Upper = energy_mJ["(Intercept)", "CI_Upper"],
  SE = energy_mJ["(Intercept)", "SE"],
  ParamLikelihood = energy_mJ["(Intercept)", "ParamLikelihood"],
  GSI = ref_location,
  Estimated_TotalEnergy = intercept_TE
)

# Step 5: Combine
GSI_TE <- rbind(ref_row3, GSI_TE)
rownames(GSI_TE) <- NULL
GSI_TE <- GSI_TE[, c("GSI", setdiff(names(GSI_TE), "GSI"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Group_Assignment)
avg_work_TE <- WorkData %>%
  dplyr::group_by(Group_Assignment) %>%
  dplyr::summarise(Avg_Work = mean(Work_km2, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
GSI_TE <- merge(GSI_TE, avg_work_TE, by.x = "GSI", by.y = "Group_Assignment") 
str(GSI_TE) #final total energy dataframe for work regressions



#================== Now, make figures of regressions for early migratory work ===================
str(GSI_body)
str(GSI_ED)
str(GSI_TE)


GSI_body <- GSI_body %>% mutate(Type = "Body size", 
                                Estimate = Estimated_BodySize)
GSI_ED   <- GSI_ED %>% mutate(Type = "Somatic energy density", 
                              Estimate = Estimated_EnergyDensity)
GSI_TE   <- GSI_TE %>% mutate(Type = "Total somatic energy", 
                              Estimate = Estimated_TotalEnergy)
GSI_all <- bind_rows(GSI_body, GSI_ED, GSI_TE)

GSI_all$GSI<-as.factor(GSI_all$GSI)

levels(GSI_all$GSI)[levels(GSI_all$GSI) == "TanadaCopperLakes"] <- "Tanada Copper Lakes"
levels(GSI_all$GSI)[levels(GSI_all$GSI) == "GulkanaHatchery"] <- "Gulkana Hatchery"
levels(GSI_all$GSI)[levels(GSI_all$GSI) == "KlutinaTonsinaOutlet"] <- "Klutina Tonsina Outlet"
levels(GSI_all$GSI)[levels(GSI_all$GSI) == "KlutinaLake"] <- "Klutina Lake"

# Data excluding Gulkana Hatchery for regression fits (early migration)
GSI_noGulkanaHatchery <- GSI_all %>% filter(GSI != "Gulkana Hatchery")




colors_GSI <- c(
  "Bremner" = "#E0B300",   
  "Chitina" = "#FFE5CCFF", #"black" if need to remove
  "Gulkana" = "#A50021FF",   
  "Gulkana Hatchery" = "black", #"D82632FF" when included 
  "Tazlina" = "#FF5C5C",   # reddish
  "Slana" = "#CC5800FF",
  "Klutina Lake" = "#FF8E32FF", 
  "Klutina Tonsina Outlet" = "#E56B00", 
  "Tanada Copper Lakes" = "#993F00FF")

GSI_all$GSI <- factor(GSI_all$GSI,
                 levels = c("Bremner",
                            "Chitina",
                            "Klutina Tonsina Outlet",
                            "Klutina Lake",
                            "Tazlina",
                            "Slana",
                            "Gulkana",
                            "Tanada Copper Lakes",
                            "Gulkana Hatchery"))

work.condition <- ggplot() +
  # Regression lines manually colored by facet
  geom_smooth(
    data = GSI_noGulkanaHatchery %>% filter(Type == "Body size"),
    aes(x = Avg_Work, y = Estimate),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  geom_smooth(
    data = GSI_noGulkanaHatchery %>% filter(Type == "Somatic energy density"),
    aes(x = Avg_Work, y = Estimate),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  geom_smooth(
    data = GSI_noGulkanaHatchery %>% filter(Type == "Total somatic energy"),
    aes(x = Avg_Work, y = Estimate),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  )  +
  # Points for all stocks except GH
  geom_point(
    data = GSI_all %>% filter(GSI != "Gulkana Hatchery"),
    aes(x = Avg_Work, y = Estimate, color = GSI),
    shape = 16,
    size = 12
  ) +
  # GH as X
  geom_point(
    data = GSI_all %>% filter(GSI == "Gulkana Hatchery"),
    aes(x = Avg_Work, y = Estimate, color = GSI),
    shape = 4,
    size = 12,
    stroke = 1.5
  ) +
  # Facet by trait
  facet_wrap(~Type, scales = "free_y") +
  # Stock colors for points (legend)
  scale_color_manual(values = colors_GSI) +
  # Labels and theme
  labs(
    x = "Migratory Work (km²)",
    y = "Average Weighted Parameter Estimate",
    color = "Genetic Stock"
  ) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(2, "lines"),
    panel.spacing = unit(1.5, "lines"),
    # Facet header styling
    strip.text = element_text(size = 30, face = "bold", color = "white"),
    strip.background = element_rect(fill = "black", color = "black"),
    # Axis title margins
    axis.title.x = element_text(size = 32, face = "bold", margin = margin(t = 15)),
    axis.title.y = element_text(size = 32, face = "bold", margin = margin(r = 15)),
    #legend edits
    legend.title = element_text(size =28, face = "bold"),
    legend.text = element_text(size =28),
    # Axis text
    axis.text = element_text(size = 28, color = "black"),
    # Panel border
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
    # Grid tweaks
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey90")
  )






#================ Add regression stats to plots ===================
library(dplyr)
library(broom)

# Fit linear models by Type (facet) excluding GH
regression_stats <- GSI_noGulkanaHatchery %>%
  group_by(Type) %>%
  do({
    model <- lm(Estimate ~ Avg_Work, data = .)
    glance_model <- broom::glance(model)
    data.frame(
      R2 = glance_model$r.squared
    )
  }) %>%
  ungroup()

# Compute top-left coordinates per facet
# Use min x and max y within each facet (Type)
coords <- GSI_noGulkanaHatchery %>%
  group_by(Type) %>%
  summarise(
    x = min(Avg_Work, na.rm = TRUE),
    y = max(Estimate, na.rm = TRUE)
  )

# Combine with regression stats
regression_stats <- regression_stats %>%
  left_join(coords, by = "Type")

#### Add A, B, C labels to each panel
# Create a small data frame of facet labels
facet_labels <- data.frame(
  Type = c("Body size", "Somatic energy density", "Total somatic energy"),  # must match your facet variable names exactly
  label = c("a", "b", "c"),
  x = Inf,
  y = -Inf
)

# Add to plot
GSI.work.condition <- work.condition +
  geom_text(
  data = regression_stats,
  aes(
    x = -Inf,
    y = Inf,
    label = paste0("R² = ", round(R2, 2))
  ),
  inherit.aes = FALSE,
  hjust = -0.1,  # pushes inside from left
  vjust = 1.5,   # pushes down from top
  size = 10
) +
  geom_text(
    data = facet_labels,
    aes(x = x, y = y, label = label),
    hjust = 2, vjust = -1,     # push inward from the edges
    size = 18, fontface = "bold",  # adjust size/style
    inherit.aes = FALSE
  )

GSI.work.condition


ggsave(
  filename = "AICFigures/Work_GSI_Condition_RegressionPlot.jpeg",    # output file name
  plot = GSI.work.condition,              # the ggplot object
  width = 24,                     # width in inches
  height = 12,                    # height in inches
  dpi = 300                        # resolution (higher = better quality)
)
```


### Compare null condition models to work models 
```{r}
library(AICcmodavg)

# ========= body size ========
str(GSI_noGulkanaHatchery)
bodymodel1 <- lm(Estimated_BodySize ~ 1, data = GSI_noGulkanaHatchery)
bodymodel2 <- lm(Estimated_BodySize ~ Avg_Work, data = GSI_noGulkanaHatchery)
summary(bodymodel1)
summary(bodymodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(bodymodel1, bodymodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)


# ======== energy density =======
str(GSI_ED)
EDmodel1 <- lm(Estimated_EnergyDensity ~ 1, data = GSI_noGulkanaHatchery)
EDmodel2 <- lm(Estimated_EnergyDensity ~ Avg_Work, data = GSI_noGulkanaHatchery)
summary(EDmodel1)
summary(EDmodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(EDmodel1, EDmodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)

# ======= total energy =========
str(GSI_TE)
TEmodel1 <- lm(Estimated_TotalEnergy ~ 1, data = GSI_noGulkanaHatchery)
TEmodel2 <- lm(Estimated_TotalEnergy ~ Avg_Work, data = GSI_noGulkanaHatchery)
summary(TEmodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(TEmodel1, TEmodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)

```


### GSI Early Migratory Work vs fecundity Figure
```{r}
library(dplyr)
library(ggplot2)

rm(list = ls())

#Download Work Data set
WorkData <- read.csv("lowerriver_GSI_PCscores.csv")[-1]
str(WorkData)
WorkData$Group_Assignment <- as.factor(WorkData$Group_Assignment)
levels(WorkData$Group_Assignment) #levels are not all consistent. Need to fix this

# Strip whitespace
WorkData$Group_Assignment <- trimws(WorkData$Group_Assignment)

# Standardize levels
WorkData$Group_Assignment <- factor(WorkData$Group_Assignment)
levels(WorkData$Group_Assignment)

# Clean up levels manually
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "Gulkana "] <- "Gulkana"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "Gulkanahatchery"] <- "GulkanaHatchery"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "KlutinaTonsinaOutlets"] <- "KlutinaTonsinaOutlet"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "insufficient genotypes"] <- "insufficient_genotypes"

# Then drop the insufficient genotypes
WorkData <- subset(WorkData, Group_Assignment != "insufficient_genotypes")
WorkData <- subset(WorkData, Group_Assignment != "LowerGroups")
WorkData$Group_Assignment <- droplevels(WorkData$Group_Assignment)



#download coefficient datasets for body size, energy density, total energy
fecundity <- read.csv("fecundity_GSI/fecundity_GSI_ModelAvg_Coefs_withLikelihoods.csv", row.names = 1)



#================ fecundity estimate calculations according to intercept =============
# Step 1: Extract intercept
str(fecundity)
intercept_fecundity <- fecundity["(Intercept)", "Avg_Coef"]

# Step 2: Extract Group_Assignment effects
GSI_fecundity <- fecundity[grep("^Group_Assignment", rownames(fecundity)), ]
GSI_fecundity <- GSI_fecundity[!grepl(":", rownames(GSI_fecundity)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
GSI_fecundity$GSI <- gsub("Group_Assignment", "", rownames(GSI_fecundity))

# Step 3: Add intercept to each to get estimated fecundity
GSI_fecundity$Estimated_Fecundity <- intercept_fecundity + GSI_fecundity$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Group_Assignment)[1]  # Assumes first level is the reference
ref_row <- data.frame(
  Avg_Coef = intercept_fecundity,
  CI_Lower = fecundity["(Intercept)", "CI_Lower"],
  CI_Upper = fecundity["(Intercept)", "CI_Upper"],
  SE = fecundity["(Intercept)", "SE"],
  ParamLikelihood = fecundity["(Intercept)", "ParamLikelihood"],
  GSI = ref_location,
  Estimated_Fecundity = intercept_fecundity
)

# Step 5: Combine
GSI_fecundity <- rbind(ref_row, GSI_fecundity)
rownames(GSI_fecundity) <- NULL
GSI_fecundity <- GSI_fecundity[, c("GSI", setdiff(names(GSI_fecundity), "GSI"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Group_Assignment)
avg_work_fecundity <- WorkData %>%
  dplyr::group_by(Group_Assignment) %>%
  dplyr::summarise(Avg_Work = mean(Work_km2, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
GSI_fecundity <- merge(GSI_fecundity, avg_work_fecundity, by.x = "GSI", by.y = "Group_Assignment") 
str(GSI_fecundity) #final fecundity dataframe for work regressions




#================== Now, make figures of regressions for early migratory work ===================
str(GSI_fecundity)

GSI_fecundity$GSI<-as.factor(GSI_fecundity$GSI)

levels(GSI_fecundity$GSI)[levels(GSI_fecundity$GSI) == "TanadaCopperLakes"] <- "Tanada Copper Lakes"
levels(GSI_fecundity$GSI)[levels(GSI_fecundity$GSI) == "GulkanaHatchery"] <- "Gulkana Hatchery"
levels(GSI_fecundity$GSI)[levels(GSI_fecundity$GSI) == "KlutinaTonsinaOutlet"] <- "Klutina Tonsina Outlet"
levels(GSI_fecundity$GSI)[levels(GSI_fecundity$GSI) == "KlutinaLake"] <- "Klutina Lake"

GSI_fecundity$GSI <- factor(GSI_fecundity$GSI,
                 levels = c("Bremner",
                            "Chitina",
                            "Klutina Tonsina Outlet",
                            "Klutina Lake",
                            "Tazlina",
                            "Slana",
                            "Gulkana",
                            "Tanada Copper Lakes",
                            "Gulkana Hatchery"))

# Data excluding Gulkana Hathcery for regression fits
GSI_noGulkanaHatchery <- GSI_fecundity %>% filter(GSI != "Gulkana Hatchery")

#fecundity color is = "#0072B2"

colors_GSI <- c(
  "Bremner" = "#E0B300",   
  "Chitina" = "#FFE5CCFF", #"black" if need to remove 
  "Gulkana" = "#A50021FF",   
  "Gulkana Hatchery" = "black", #D32826FF when included 
  "Tazlina" = "#FF5C5C",   # reddish
  "Slana" = "#CC5800FF",
  "Klutina Lake" = "#FF8E32FF", 
  "Klutina Tonsina Outlet" = "#E56B00", 
  "Tanada Copper Lakes" = "#993F00FF")

GSI_fecundity$Facet <- "Fecundity"

work.fecundity <- ggplot() +
  # Regression lines manually colored by facet
  geom_smooth(
    data = GSI_noGulkanaHatchery,
    aes(x = Avg_Work, y = Estimated_Fecundity),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  # Points for all stocks except Chitina
  geom_point(
    data = GSI_fecundity %>% filter(GSI != "Gulkana Hatchery"),
    aes(x = Avg_Work, y = Estimated_Fecundity, color = GSI),
    shape = 16,
    size = 12
  ) +
  # Chitina as X
  geom_point(
    data = GSI_fecundity %>% filter(GSI == "Gulkana Hatchery"),
    aes(x = Avg_Work, y = Estimated_Fecundity, color = GSI),
    shape = 4,
    size = 12,
    stroke = 1.5
  ) +
  scale_color_manual(values = colors_GSI) +
  # Labels and theme
  labs(
    x = "Migratory Work (km²)",
    y = "Average Weighted Parameter Estimate",
    color = "Genetic Stock"
  ) +
  facet_wrap(~Facet, strip.position = "top") +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(2, "lines"),
    panel.spacing = unit(1.5, "lines"),
    # Facet header styling
    strip.text = element_text(size = 30, face = "bold", color = "white"),
    strip.background = element_rect(fill = "black", color = "black"),
    # Axis title margins
    axis.title.x = element_text(size = 30, face = "bold", margin = margin(t = 15)),
    axis.title.y = element_text(size = 30, face = "bold", margin = margin(r = 15)),
    #legend edits
    legend.title = element_text(size =24, face = "bold"),
    legend.text = element_text(size =24),
    # Axis text
    axis.text = element_text(size = 24, color = "black"),
    # Panel border
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
    # Grid tweaks
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey90")
  )






#================ Add regression stats to plots ===================
library(dplyr)
library(broom)

# Fit linear model excluding Chitina
fec_model <- lm(Estimated_Fecundity ~ Avg_Work, data = GSI_noGulkanaHatchery)


# 2. Extract R² 
model_stats <- broom::glance(fec_model) %>%
  mutate(
    label = sprintf("R² = %.2f", r.squared),
    x = min(GSI_noGulkanaHatchery$Avg_Work),
    y = max(GSI_noGulkanaHatchery$Estimated_Fecundity)
  )
# 3. Add the label to your plot
work.fecundity1 <- work.fecundity +
  geom_text(
    data = model_stats,
    aes(x = x, y = y, label = label),
    hjust = 0, vjust = -4,    # top-left corner
    size = 10
  )

work.fecundity1



ggsave(
  filename = "AICFigures/Work_GSI_Fecundity_RegressionPlot.jpeg",    # output file name
  plot = work.fecundity1,              # the ggplot object
  width = 18,                     # width in inches
  height = 12,                    # height in inches
  dpi = 300                        # resolution (higher = better quality)
)


```

### Compare null condition models to fecundity GSI work models 
```{r}
library(AICcmodavg)

# ========= fecundity ========
str(GSI_fecundity)
fecmodel1 <- lm(Estimated_Fecundity ~ 1, data = GSI_noGulkanaHatchery)
fecmodel2 <- lm(Estimated_Fecundity ~ Avg_Work, data = GSI_noGulkanaHatchery)
summary(fecmodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(fecmodel1, fecmodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)


```


### GSI Early Migratory Work vs Fineness Figure
```{r}
library(dplyr)
library(ggplot2)

rm(list = ls())

#Download Work Data set
WorkData <- read.csv("lowerriver_GSI_PCscores.csv")[-1]
str(WorkData)
WorkData$Group_Assignment <- as.factor(WorkData$Group_Assignment)
levels(WorkData$Group_Assignment) #levels are not all consistent. Need to fix this

# Strip whitespace
WorkData$Group_Assignment <- trimws(WorkData$Group_Assignment)

# Standardize levels
WorkData$Group_Assignment <- factor(WorkData$Group_Assignment)
levels(WorkData$Group_Assignment)

# Clean up levels manually
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "Gulkana "] <- "Gulkana"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "Gulkanahatchery"] <- "GulkanaHatchery"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "KlutinaTonsinaOutlets"] <- "KlutinaTonsinaOutlet"
levels(WorkData$Group_Assignment)[levels(WorkData$Group_Assignment) == "insufficient genotypes"] <- "insufficient_genotypes"

# Then drop the insufficient genotypes
WorkData <- subset(WorkData, Group_Assignment != "insufficient_genotypes")
WorkData <- subset(WorkData, Group_Assignment != "LowerGroups")
WorkData$Group_Assignment <- droplevels(WorkData$Group_Assignment)



#download coefficient datasets for body size, energy density, total energy
fineness <- read.csv("fineness_GSI/fineness_GSI_ModelAvg_Coefs_withLikelihoods.csv", row.names = 1)



#================ fineness estimate calculations according to intercept =============
# Step 1: Extract intercept
str(fineness)
intercept_fineness <- fineness["(Intercept)", "Avg_Coef"]

# Step 2: Extract Group_Assignment effects
GSI_fineness <- fineness[grep("^Group_Assignment", rownames(fineness)), ]
GSI_fineness <- GSI_fineness[!grepl(":", rownames(GSI_fineness)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
GSI_fineness$GSI <- gsub("Group_Assignment", "", rownames(GSI_fineness))

# Step 3: Add intercept to each to get estimated fineness
GSI_fineness$Estimated_Fineness <- intercept_fineness + GSI_fineness$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Group_Assignment)[1]  # Assumes first level is the reference
ref_row <- data.frame(
  Avg_Coef = intercept_fineness,
  CI_Lower = fineness["(Intercept)", "CI_Lower"],
  CI_Upper = fineness["(Intercept)", "CI_Upper"],
  SE = fineness["(Intercept)", "SE"],
  ParamLikelihood = fineness["(Intercept)", "ParamLikelihood"],
  GSI = ref_location,
  Estimated_Fineness = intercept_fineness
)

# Step 5: Combine
GSI_fineness <- rbind(ref_row, GSI_fineness)
rownames(GSI_fineness) <- NULL
GSI_fineness <- GSI_fineness[, c("GSI", setdiff(names(GSI_fineness), "GSI"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Group_Assignment)
avg_work_fineness <- WorkData %>%
  dplyr::group_by(Group_Assignment) %>%
  dplyr::summarise(Avg_Work = mean(Work_km2, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
GSI_fineness <- merge(GSI_fineness, avg_work_fineness, by.x = "GSI", by.y = "Group_Assignment") 
str(GSI_fineness) #final fecundity dataframe for work regressions




#================== Now, make figures of regressions for early migratory work ===================
str(GSI_fineness)

GSI_fineness$GSI<-as.factor(GSI_fineness$GSI)

levels(GSI_fineness$GSI)[levels(GSI_fineness$GSI) == "TanadaCopperLakes"] <- "Tanada Copper Lakes"
levels(GSI_fineness$GSI)[levels(GSI_fineness$GSI) == "GulkanaHatchery"] <- "Gulkana Hatchery"
levels(GSI_fineness$GSI)[levels(GSI_fineness$GSI) == "KlutinaTonsinaOutlet"] <- "Klutina Tonsina Outlet"
levels(GSI_fineness$GSI)[levels(GSI_fineness$GSI) == "KlutinaLake"] <- "Klutina Lake"

GSI_fineness$GSI <- factor(GSI_fineness$GSI,
                 levels = c("Bremner",
                            "Chitina",
                            "Klutina Tonsina Outlet",
                            "Klutina Lake",
                            "Tazlina",
                            "Slana",
                            "Gulkana",
                            "Tanada Copper Lakes",
                            "Gulkana Hatchery"))

# Data excluding Gulkana Hathcery for regression fits
GSI_noGulkanaHatchery <- GSI_fineness %>% filter(GSI != "Gulkana Hatchery")

#fecundity color is = "#0072B2"

colors_GSI <- c(
  "Bremner" = "#E0B300",   
  "Chitina" = "#FFE5CCFF", #"black" if need to remove 
  "Gulkana" = "#A50021FF",   
  "Gulkana Hatchery" = "black", #D32826FF when included 
  "Tazlina" = "#FF5C5C",   # reddish
  "Slana" = "#CC5800FF",
  "Klutina Lake" = "#FF8E32FF", 
  "Klutina Tonsina Outlet" = "#E56B00", 
  "Tanada Copper Lakes" = "#993F00FF")

GSI_fineness$Facet <- "Fineness"

work.fineness <- ggplot() +
  # Regression lines manually colored by facet
  geom_smooth(
    data = GSI_noGulkanaHatchery,
    aes(x = Avg_Work, y = Estimated_Fineness),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  # Points for all stocks except Chitina
  geom_point(
    data = GSI_fineness %>% filter(GSI != "Gulkana Hatchery"),
    aes(x = Avg_Work, y = Estimated_Fineness, color = GSI),
    shape = 16,
    size = 12
  ) +
  # Chitina as X
  geom_point(
    data = GSI_fineness %>% filter(GSI == "Gulkana Hatchery"),
    aes(x = Avg_Work, y = Estimated_Fineness, color = GSI),
    shape = 4,
    size = 12,
    stroke = 1.5
  ) +
  scale_color_manual(values = colors_GSI) +
  # Labels and theme
  labs(
    x = "Migratory Work (km²)",
    y = "Average Weighted Parameter Estimate",
    color = "Genetic Stock"
  ) +
  facet_wrap(~Facet, strip.position = "top") +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(2, "lines"),
    panel.spacing = unit(1.5, "lines"),
    # Facet header styling
    strip.text = element_text(size = 30, face = "bold", color = "white"),
    strip.background = element_rect(fill = "black", color = "black"),
    # Axis title margins
    axis.title.x = element_text(size = 30, face = "bold", margin = margin(t = 15)),
    axis.title.y = element_text(size = 30, face = "bold", margin = margin(r = 15)),
    #legend edits
    legend.title = element_text(size =24, face = "bold"),
    legend.text = element_text(size =24),
    # Axis text
    axis.text = element_text(size = 24, color = "black"),
    # Panel border
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
    # Grid tweaks
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey90")
  )

work.fineness




#================ Add regression stats to plots ===================
library(dplyr)
library(broom)

# Fit linear model excluding Chitina
fineness_model <- lm(Estimated_Fineness ~ Avg_Work, data = GSI_noGulkanaHatchery)


# 2. Extract R² 
model_stats <- broom::glance(fineness_model) %>%
  mutate(
    label = paste0("R² = ", round(r.squared, 3)),
    x = min(GSI_noGulkanaHatchery$Avg_Work),   # x position for label
    y = max(GSI_noGulkanaHatchery$Estimated_Fineness) # y position for label
  )

# 3. Add the label to your plot
work.fineness1 <- work.fineness +
  geom_text(
    data = model_stats,
    aes(x = x, y = y, label = label),
    hjust = 0, vjust = -2,    # top-left corner
    size = 10
  )

work.fineness1 #no figure to save as it is not supported more than null!

```


### Compare null condition models to fineness GSI work models 
```{r}
library(AICcmodavg)

# ========= fecundity ========
str(GSI_fineness)
finmodel1 <- lm(Estimated_Fineness ~ 1, data = GSI_noGulkanaHatchery)
finmodel2 <- lm(Estimated_Fineness ~ Avg_Work, data = GSI_noGulkanaHatchery)
summary(finmodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(finmodel1, finmodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)


```




### Spawnning Population Post Migratory Work vs condition figures 
```{r}
library(dplyr)
library(ggplot2)

rm(list = ls())

#Download Work Data set
WorkData <- read.csv("upperriver_bodycomp_PCscores.csv")[-1]
str(WorkData)
WorkData$Collection_Location <- as.factor(WorkData$Collection_Location)
levels(WorkData$Collection_Location)
#Make work metric (migratory difficulty metric where work = F x distance)
WorkData$Elevation_m <- as.numeric(WorkData$Elevation_m)
WorkData$Work <- round((WorkData$Collection_RiverMile_m * WorkData$Elevation_m) / (1000 * 1000), digits = 1) #in km^2
str(WorkData) 


#download coefficient datasets for body size, energy density, total energy
body <- read.csv("bodysize_spawningpopulation/bodysize_spawningpopulation_ModelAvg_Coefs_withLikelihoods.csv", row.names = 1)
energy_mJkg <- read.csv("energydensity_spawningpopulation/energydensity_spawningpopulation_ModelAvg_Coefs_withLikelihoods.csv", row.names = 1)
energy_mJ <- read.csv("totalenergy2_spawningpopulation/TotalEnergy_SP_ModelAvg_Coefs_withLikelihoods.csv", row.names = 1)

#================ body size estimate calculations according to intercept =============
# Step 1: Extract intercept
str(body)
intercept_body <- body["(Intercept)", "Avg_Coef"]

# Step 2: Extract Group_Assignment effects
SP_GSI_body <- body[grep("^Collection_Location", rownames(body)), ]
SP_GSI_body <- SP_GSI_body[!grepl(":", rownames(SP_GSI_body)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
SP_GSI_body$Location <- gsub("Collection_Location", "", rownames(SP_GSI_body))

# Step 3: Add intercept to each to get estimated body size
SP_GSI_body$Estimated_BodySize <- intercept_body + SP_GSI_body$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Collection_Location)[1]  # Assumes first level is the reference
ref_row <- data.frame(
  Avg_Coef = intercept_body,
  CI_Lower = body["(Intercept)", "CI_Lower"],
  CI_Upper = body["(Intercept)", "CI_Upper"],
  SE = body["(Intercept)", "SE"],
  ParamLikelihood = body["(Intercept)", "ParamLikelihood"],
  Location = ref_location,
  Estimated_BodySize = intercept_body
)

# Step 5: Combine
SP_GSI_body <- rbind(ref_row, SP_GSI_body)
rownames(SP_GSI_body) <- NULL
SP_GSI_body <- SP_GSI_body[, c("Location", setdiff(names(SP_GSI_body), "Location"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Collection_Location) #7 unique
avg_work <- WorkData %>%
  dplyr::group_by(Collection_Location) %>%
  dplyr::summarise(Avg_Work = mean(Work, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
SP_GSI_body <- merge(SP_GSI_body, avg_work, by.x = "Location", by.y = "Collection_Location")

str(SP_GSI_body) #final body size dataframe for work regressions





#================ energy density estimate calculations according to intercept =============
# Step 1: Extract intercept
str(energy_mJkg)
intercept_ED <- energy_mJkg["(Intercept)", "Avg_Coef"]

# Step 2: Extract collection location effects
SP_GSI_ED <- energy_mJkg[grep("^Collection_Location", rownames(energy_mJkg)), ]
SP_GSI_ED <- SP_GSI_ED[!grepl(":", rownames(SP_GSI_ED)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
SP_GSI_ED$Location <- gsub("Collection_Location", "", rownames(SP_GSI_ED))

# Step 3: Add intercept to each to get estimated energy density
SP_GSI_ED$Estimated_EnergyDensity <- intercept_ED + SP_GSI_ED$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Collection_Location)[1]  # Assumes first level is the reference
ref_row <- data.frame(
  Avg_Coef = intercept_ED,
  CI_Lower = energy_mJkg["(Intercept)", "CI_Lower"],
  CI_Upper = energy_mJkg["(Intercept)", "CI_Upper"],
  SE = energy_mJkg["(Intercept)", "SE"],
  ParamLikelihood = energy_mJkg["(Intercept)", "ParamLikelihood"],
  Location = ref_location,
  Estimated_EnergyDensity = intercept_ED
)

# Step 5: Combine
SP_GSI_ED <- rbind(ref_row, SP_GSI_ED)
rownames(SP_GSI_ED) <- NULL
SP_GSI_ED <- SP_GSI_ED[, c("Location", setdiff(names(SP_GSI_ED), "Location"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Collection_Location) #7 unique
avg_work <- WorkData %>%
  dplyr::group_by(Collection_Location) %>%
  dplyr::summarise(Avg_Work = mean(Work, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
SP_GSI_ED <- merge(SP_GSI_ED, avg_work, by.x = "Location", by.y = "Collection_Location")

str(SP_GSI_ED) #final energy density data set for work regressions



#================ total energy estimate calculations according to intercept =============
# Step 1: Extract intercept
str(energy_mJ)
intercept_TE <- energy_mJ["(Intercept)", "Avg_Coef"]

# Step 2: Extract collection location effects
SP_GSI_TE <- energy_mJ[grep("^Collection_Location", rownames(energy_mJ)), ]
SP_GSI_TE <- SP_GSI_TE[!grepl(":", rownames(SP_GSI_TE)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
SP_GSI_TE$Location <- gsub("Collection_Location", "", rownames(SP_GSI_TE))

# Step 3: Add intercept to each to get estimated energy density
SP_GSI_TE$Estimated_TotalEnergy <- intercept_TE + SP_GSI_TE$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Collection_Location)[1]  # Assumes first level is the reference
ref_row <- data.frame(
  Avg_Coef = intercept_TE,
  CI_Lower = energy_mJ["(Intercept)", "CI_Lower"],
  CI_Upper = energy_mJ["(Intercept)", "CI_Upper"],
  SE = energy_mJ["(Intercept)", "SE"],
  ParamLikelihood = energy_mJ["(Intercept)", "ParamLikelihood"],
  Location = ref_location,
  Estimated_TotalEnergy = intercept_TE
)

# Step 5: Combine
SP_GSI_TE <- rbind(ref_row, SP_GSI_TE)
rownames(SP_GSI_TE) <- NULL
SP_GSI_TE <- SP_GSI_TE[, c("Location", setdiff(names(SP_GSI_TE), "Location"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Collection_Location) #7 unique
avg_work <- WorkData %>%
  dplyr::group_by(Collection_Location) %>%
  dplyr::summarise(Avg_Work = mean(Work, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
SP_GSI_TE <- merge(SP_GSI_TE, avg_work, by.x = "Location", by.y = "Collection_Location")

str(SP_GSI_TE) #final energy density data set for work regressions





#================== Now, make figures of regressions for early migratory work ===================
str(SP_GSI_body)
str(SP_GSI_ED)
str(SP_GSI_TE)


SP_GSI_body <- SP_GSI_body %>% mutate(Type = "Body size", 
                                Estimate = Estimated_BodySize)
SP_GSI_ED   <- SP_GSI_ED %>% mutate(Type = "Somatic energy density", 
                              Estimate = Estimated_EnergyDensity)
SP_GSI_TE   <- SP_GSI_TE %>% mutate(Type = "Total somatic energy", 
                              Estimate = Estimated_TotalEnergy)
SP_GSI_all <- bind_rows(SP_GSI_body, SP_GSI_ED, SP_GSI_TE)

# Data excluding Chitina for regression fits
SP_noLongLake <- SP_GSI_all %>% filter(Location != "Long Lake")
SP_noLLnoGH <- SP_noLongLake %>% filter(Location != "Gulkana Hatchery")

SP_GSI_all$Location <- factor(SP_GSI_all$Location,
                 levels = c("Long Lake",
                            "St Anne",
                            "Mahlo",
                            "Mentasta",
                            "Tanada",
                            "Gulkana Hatchery",
                            "Fish Creek Gulkana"))

colors_SP <- c(
  "Long Lake" = "black", #FFE5CCFF" when included
  "Fish Creek Gulkana" = "#A50021FF",   
  "Gulkana Hatchery" = "#D82632FF", 
  "St Anne" = "#FFAD65FF",   # reddish
  "Mahlo" = "#FF8E32FF",
  "Mentasta" = "#CC5800FF", 
  "Tanada" = "#993F00FF")

work.condition <- ggplot() +
  # Regression lines manually colored by facet
  geom_smooth(
    data = SP_noLLnoGH %>% filter(Type == "Body size"),
    aes(x = Avg_Work, y = Estimate),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  geom_smooth(
    data = SP_noLLnoGH %>% filter(Type == "Somatic energy density"),
    aes(x = Avg_Work, y = Estimate),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  geom_smooth(
    data = SP_noLLnoGH %>% filter(Type == "Total somatic energy"),
    aes(x = Avg_Work, y = Estimate),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  )  +
  # Points for all stocks except Chitina
  geom_point(
    data = SP_GSI_all %>% filter(!Location %in% c("Long Lake", "Gulkana Hatchery")),
    aes(x = Avg_Work, y = Estimate, color = Location),
    shape = 16,
    size = 12
) +
  geom_point(
    data = SP_GSI_all %>% filter(Location %in% c("Long Lake", "Gulkana Hatchery")),
    aes(x = Avg_Work, y = Estimate, color = Location),
    shape = 4,
    size = 12,
    stroke = 1.5
) +
  # Facet by trait
  facet_wrap(~Type, scales = "free_y") +
  # Stock colors for points (legend)
  scale_color_manual(values = colors_SP) +
  # Labels and theme
  labs(
    x = "Migratory Work (km²)",
    y = "Average Weighted Parameter Estimate",
    color = "Spawning Population"
  ) +  scale_x_continuous(expand = expansion(mult = c(0.10, 0.10))) +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(2, "lines"),
    panel.spacing = unit(1.5, "lines"),
    # Facet header styling
    strip.text = element_text(size = 30, face = "bold", color = "white"),
    strip.background = element_rect(fill = "black", color = "black"),
    # Axis title margins
    axis.title.x = element_text(size = 32, face = "bold", margin = margin(t = 15)),
    axis.title.y = element_text(size = 32, face = "bold", margin = margin(r = 15)),
    #legend edits
    legend.title = element_text(size =28, face = "bold"),
    legend.text = element_text(size =28),
    # Axis text
    axis.text = element_text(size = 28, color = "black"),
    # Panel border
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
    # Grid tweaks
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey90")
  )




#================ Add regression stats to plots ===================
library(dplyr)
library(broom)

# Fit linear models by Type (facet) excluding GH
regression_stats <- SP_noLLnoGH %>%
  group_by(Type) %>%
  do({
    model <- lm(Estimate ~ Avg_Work, data = .)
    glance_model <- broom::glance(model)
    data.frame(
      R2 = glance_model$r.squared
    )
  }) %>%
  ungroup()

# Compute top-left coordinates per facet
# Use min x and max y within each facet (Type)
coords <- SP_noLLnoGH %>%
  group_by(Type) %>%
  summarise(
    x = min(Avg_Work, na.rm = TRUE),
    y = max(Estimate, na.rm = TRUE)
  )

# Combine with regression stats
regression_stats <- regression_stats %>%
  left_join(coords, by = "Type")

#### Add A, B, C labels to each panel
# Create a small data frame of facet labels
facet_labels <- data.frame(
  Type = c("Body size", "Somatic energy density", "Total somatic energy"),  # must match your facet variable names exactly
  label = c("a", "b", "c"),
  x = Inf,
  y = -Inf
)

# Add to plot
SP.work.condition <- work.condition +
  geom_text(
  data = regression_stats,
  aes(
    x = -Inf,
    y = Inf,
    label = paste0("R² = ", round(R2, 2))
  ),
  inherit.aes = FALSE,
  hjust = -0.1,  # pushes inside from left
  vjust = 1.5,   # pushes down from top
  size = 10
) +
  geom_text(
    data = facet_labels,
    aes(x = x, y = y, label = label),
    hjust = 2, vjust = -1,     # push inward from the edges
    size = 18, fontface = "bold",  # adjust size/style
    inherit.aes = FALSE
  )

SP.work.condition

ggsave(
  filename = "AICFigures/Work_SP_Condition_RegressionPlot.jpeg",    # output file name
  plot = SP.work.condition,              # the ggplot object
  width = 24,                     # width in inches
  height = 12,                    # height in inches
  dpi = 300                        # resolution (higher = better quality)
)


```


### Compare null condition model with condition ~ SP work model

```{r}
library(AICcmodavg)

# ========= body size ========
str(SP_noLLnoGH)
bodymodel1 <- lm(Estimated_BodySize ~ 1, data = SP_noLLnoGH)
bodymodel2 <- lm(Estimated_BodySize ~ Avg_Work, data = SP_noLLnoGH)
summary(bodymodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(bodymodel1, bodymodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)


# ======== energy density =======
str(SP_noLLnoGH)
EDmodel1 <- lm(Estimated_EnergyDensity ~ 1, data = SP_noLLnoGH)
EDmodel2 <- lm(Estimated_EnergyDensity ~ Avg_Work, data = SP_noLLnoGH)
summary(EDmodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(EDmodel1, EDmodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)



# ======= total energy =========
str(SP_noLLnoGH)
TEmodel1 <- lm(Estimated_TotalEnergy ~ 1, data = SP_noLLnoGH)
TEmodel2 <- lm(Estimated_TotalEnergy ~ Avg_Work, data = SP_noLLnoGH)
summary(TEmodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(TEmodel1, TEmodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)

```




### Spawning Population Post-Migratory Work vs egg size Figure
```{r}
library(dplyr)
library(ggplot2)

rm(list = ls())

WorkData <- read.csv("upperriver_bodycomp_PCscores.csv")[-1]
str(WorkData)
WorkData$Collection_Location <- as.factor(WorkData$Collection_Location)
levels(WorkData$Collection_Location)
#Make work metric (migratory difficulty metric where work = F x distance)
WorkData$Elevation_m <- as.numeric(WorkData$Elevation_m)
WorkData$Work <- round((WorkData$Collection_RiverMile_m * WorkData$Elevation_m) / (1000 * 1000), digits = 1) #in km^2
str(WorkData) 


#download coefficient datasets for egg size
egg <- read.csv("eggsize_spawningpopulation/eggsize~spawningpopulation_models_CoefEstimates_withLikelihoods.csv", row.names = 1)



#================ fecundity estimate calculations according to intercept =============
# Step 1: Extract intercept
str(egg)
intercept_egg <- egg["(Intercept)", "Avg_Coef"]

# Step 2: Extract Group_Assignment effects
SP_egg <- egg[grep("^Collection_Location", rownames(egg)), ]
SP_egg <- SP_egg[!grepl(":", rownames(SP_egg)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
SP_egg$Location <- gsub("Collection_Location", "", rownames(SP_egg))

# Step 3: Add intercept to each to get estimated fecundity
SP_egg$Estimated_EggSize <- intercept_egg + SP_egg$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Collection_Location)[1]  # Assumes first level is the reference
ref_row <- data.frame(
  Avg_Coef = intercept_egg,
  CI_Lower = egg["(Intercept)", "CI_Lower"],
  CI_Upper = egg["(Intercept)", "CI_Upper"],
  SE = egg["(Intercept)", "SE"],
  ParamLikelihood = egg["(Intercept)", "ParamLikelihood"],
  Location = ref_location,
  Estimated_EggSize = intercept_egg
)

# Step 5: Combine
SP_egg <- rbind(ref_row, SP_egg)
rownames(SP_egg) <- NULL
SP_egg <- SP_egg[, c("Location", setdiff(names(SP_egg), "Location"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Collection_Location)
avg_work_egg <- WorkData %>%
  dplyr::group_by(Collection_Location) %>%
  dplyr::summarise(Avg_Work = mean(Work, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
SP_egg <- merge(SP_egg, avg_work_egg, by.x = "Location", by.y = "Collection_Location")
str(SP_egg) #final fecundity dataframe for work regressions




#================== Now, make figures of regressions for early migratory work ===================
str(SP_egg)
SP_egg$Location <- factor(SP_egg$Location,
                 levels = c("Long Lake",
                            "St Anne",
                            "Mahlo",
                            "Mentasta",
                            "Tanada",
                            "Gulkana Hatchery",
                            "Fish Creek Gulkana"))

# Data excluding Gulkana Hathcery for regression fits
SP_noLLnoGH <- SP_egg %>%
  filter(!Location %in% c("Gulkana Hatchery", "Long Lake"))

#fecundity color is = "#0072B2"

colors_SP <- c(
  "Long Lake" = "black", #FFE5CCFF" when included
  "Fish Creek Gulkana" = "#A50021FF",   
  "Gulkana Hatchery" = "#D82632FF", 
  "St Anne" = "#FFAD65FF",   # reddish
  "Mahlo" = "#FF8E32FF",
  "Mentasta" = "#CC5800FF", 
  "Tanada" = "#993F00FF")

SP_egg$Facet <- "Egg Size"

eggmodel <- lm(SP_noLLnoGH$Estimated_EggSize ~ SP_noLLnoGH$Avg_Work)
summary(eggmodel)

work.egg <- ggplot() +
  # Regression lines manually colored by facet
  geom_smooth(
    data = SP_noLLnoGH,
    aes(x = Avg_Work, y = Estimated_EggSize),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  # Points for all stocks except Chitina and GH
   geom_point(
    data = SP_egg %>% filter(!Location %in% c("Long Lake", "Gulkana Hatchery")),
    aes(x = Avg_Work, y = Estimated_EggSize, color = Location),
    shape = 16,
    size = 12
) +
  geom_point(
    data = SP_egg %>% filter(Location %in% c("Long Lake", "Gulkana Hatchery")),
    aes(x = Avg_Work, y = Estimated_EggSize, color = Location),
    shape = 4,
    size = 12,
    stroke = 1.5
) +
  scale_color_manual(values = colors_SP) +
  # Labels and theme
  labs(
    x = "Migratory Work (km²)",
    y = "Average Weighted Parameter Estimate",
    color = "Location"
  ) +
  facet_wrap(~Facet, strip.position = "top") +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(2, "lines"),
    panel.spacing = unit(1.5, "lines"),
    # Facet header styling
    strip.text = element_text(size = 30, face = "bold", color = "white"),
    strip.background = element_rect(fill = "black", color = "black"),
    # Axis title margins
    axis.title.x = element_text(size = 30, face = "bold", margin = margin(t = 15)),
    axis.title.y = element_text(size = 30, face = "bold", margin = margin(r = 15)),
    #legend edits
    legend.title = element_text(size =24, face = "bold"),
    legend.text = element_text(size =24),
    # Axis text
    axis.text = element_text(size = 24, color = "black"),
    # Panel border
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
    # Grid tweaks
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey90")
  )






#================ Add regression stats to plots ===================
library(dplyr)
library(broom)

# Fit linear model excluding Chitina
egg_model <- lm(Estimated_EggSize ~ Avg_Work, data = SP_noLLnoGH)


# 2. Extract R² 
model_stats <- broom::glance(egg_model) %>%
  mutate(
    label = paste0("R² = ", round(r.squared, 3)),
    x = min(SP_noLLnoGH$Avg_Work),   # x position for label
    y = max(SP_noLLnoGH$Estimated_EggSize) # y position for label
  )

# 3. Add the label to your plot
work.egg1 <- work.egg +
  geom_text(
    data = model_stats,
    aes(x = x, y = y, label = label),
    hjust = 1, vjust = -3,    # top-left corner
    size = 10
  )

work.egg1



ggsave(
  filename = "AICFigures/Work_SP_EggSize_RegressionPlot.jpeg",    # output file name
  plot = work.egg1,              # the ggplot object
  width = 18,                     # width in inches
  height = 12,                    # height in inches
  dpi = 300                        # resolution (higher = better quality)
)


```


### Compare null condition models to fecundity SP work models 
```{r}
library(AICcmodavg)

# ========= body size ========
str(SP_noLLnoGH)
eggmodel1 <- lm(Estimated_EggSize ~ 1, data = SP_noLLnoGH)
eggmodel2 <- lm(Estimated_EggSize ~ Avg_Work, data = SP_noLLnoGH)
summary(eggmodel2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(eggmodel1, eggmodel2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)


```






### Spawning Population Post-Migratory Work vs fineness Figure
```{r}
library(dplyr)
library(ggplot2)

rm(list = ls())

WorkData <- read.csv("upperriver_bodycomp_PCscores.csv")[-1]
str(WorkData)
WorkData$Collection_Location <- as.factor(WorkData$Collection_Location)
levels(WorkData$Collection_Location)
#Make work metric (migratory difficulty metric where work = F x distance)
WorkData$Elevation_m <- as.numeric(WorkData$Elevation_m)
WorkData$Work <- round((WorkData$Collection_RiverMile_m * WorkData$Elevation_m) / (1000 * 1000), digits = 1) #in km^2
str(WorkData) 


#download coefficient datasets for egg size
fineness <- read.csv("fineness_spawningpopulation/fineness_spawningpopulation_ModelAvg_Coefs_WithLikelihoods.csv", row.names = 1)



#================ fecundity estimate calculations according to intercept =============
# Step 1: Extract intercept
str(fineness)
intercept_fineness <- fineness["(Intercept)", "Avg_Coef"]

# Step 2: Extract Group_Assignment effects
SP_fineness <- fineness[grep("^Collection_Location", rownames(fineness)), ]
SP_fineness <- SP_fineness[!grepl(":", rownames(SP_fineness)), ]  # remove interaction terms

# Extract location names (remove "Collection_Location" from row names)
SP_fineness$Location <- gsub("Collection_Location", "", rownames(SP_fineness))

# Step 3: Add intercept to each to get estimated fecundity
SP_fineness$Estimated_Fineness <- intercept_fineness + SP_fineness$Avg_Coef

# Step 4: Add back the reference location (intercept only)
ref_location <- levels(WorkData$Collection_Location)[1]  # Assumes first level is the reference
ref_row <- data.frame(
  Avg_Coef = intercept_fineness,
  CI_Lower = fineness["(Intercept)", "CI_Lower"],
  CI_Upper = fineness["(Intercept)", "CI_Upper"],
  SE = fineness["(Intercept)", "SE"],
  ParamLikelihood = fineness["(Intercept)", "ParamLikelihood"],
  Location = ref_location,
  Estimated_Fineness = intercept_fineness
)

# Step 5: Combine
SP_fineness <- rbind(ref_row, SP_fineness)
rownames(SP_fineness) <- NULL
SP_fineness <- SP_fineness[, c("Location", setdiff(names(SP_fineness), "Location"))]

# Step 6: Compute average Work per location
library(dplyr)

# Check unique values
unique(WorkData$Collection_Location)
avg_work_fineness <- WorkData %>%
  dplyr::group_by(Collection_Location) %>%
  dplyr::summarise(Avg_Work = mean(Work, na.rm = TRUE), .groups = "drop")

# Step 7: Merge with estimates
SP_fineness <- merge(SP_fineness, avg_work_fineness, by.x = "Location", by.y = "Collection_Location")
str(SP_fineness) #final fecundity dataframe for work regressions




#================== Now, make figures of regressions for early migratory work ===================
str(SP_fineness)
SP_fineness$Location <- factor(SP_fineness$Location,
                 levels = c("Long Lake",
                            "St Anne",
                            "Mahlo",
                            "Mentasta",
                            "Tanada",
                            "Gulkana Hatchery",
                            "Fish Creek Gulkana"))

# Data excluding Gulkana Hathcery for regression fits
SP_noLLnoGH <- SP_fineness %>%
  filter(!Location %in% c("Gulkana Hatchery", "Long Lake"))

#fecundity color is = "#0072B2"

colors_SP <- c(
  "Long Lake" = "black", #FFE5CCFF" when included
  "Fish Creek Gulkana" = "#A50021FF",   
  "Gulkana Hatchery" = "#D82632FF", 
  "St Anne" = "#FFAD65FF",   # reddish
  "Mahlo" = "#FF8E32FF",
  "Mentasta" = "#CC5800FF", 
  "Tanada" = "#993F00FF")

SP_fineness$Facet <- "Fineness"

finenessmodel <- lm(SP_noLLnoGH$Estimated_Fineness ~ SP_noLLnoGH$Avg_Work)
summary(finenessmodel)

work.fineness <- ggplot() +
  # Regression lines manually colored by facet
  geom_smooth(
    data = SP_noLLnoGH,
    aes(x = Avg_Work, y = Estimated_Fineness),
    method = "lm",
    se = TRUE,
    color = "black",
    fill = "lightgrey",
    alpha = 0.4,
    linewidth = 1.5,
    inherit.aes = FALSE,
    show.legend = FALSE
  ) +
  # Points for all stocks except Chitina and GH
   geom_point(
    data = SP_fineness %>% filter(!Location %in% c("Long Lake", "Gulkana Hatchery")),
    aes(x = Avg_Work, y = Estimated_Fineness, color = Location),
    shape = 16,
    size = 12
) +
  geom_point(
    data = SP_fineness %>% filter(Location %in% c("Long Lake", "Gulkana Hatchery")),
    aes(x = Avg_Work, y = Estimated_Fineness, color = Location),
    shape = 4,
    size = 12,
    stroke = 1.5
) +
  scale_color_manual(values = colors_SP) +
  # Labels and theme
  labs(
    x = "Migratory Work (km²)",
    y = "Average Weighted Parameter Estimate",
    color = "Location"
  ) +
  facet_wrap(~Facet, strip.position = "top") +
  theme_minimal() +
  theme(
    panel.spacing.x = unit(2, "lines"),
    panel.spacing = unit(1.5, "lines"),
    # Facet header styling
    strip.text = element_text(size = 30, face = "bold", color = "white"),
    strip.background = element_rect(fill = "black", color = "black"),
    # Axis title margins
    axis.title.x = element_text(size = 30, face = "bold", margin = margin(t = 15)),
    axis.title.y = element_text(size = 30, face = "bold", margin = margin(r = 15)),
    #legend edits
    legend.title = element_text(size =24, face = "bold"),
    legend.text = element_text(size =24),
    # Axis text
    axis.text = element_text(size = 24, color = "black"),
    # Panel border
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1),
    # Grid tweaks
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey90")
  )

work.fineness




#================ Add regression stats to plots ===================
library(dplyr)
library(broom)

# Fit linear model excluding Chitina
fineness_model <- lm(Estimated_Fineness ~ Avg_Work, data = SP_noLLnoGH)


# 2. Extract R² 
model_stats <- broom::glance(fineness_model) %>%
  mutate(
    label = paste0("R² = ", round(r.squared, 3)),
    x = min(SP_noLLnoGH$Avg_Work),   # x position for label
    y = max(SP_noLLnoGH$Estimated_Fineness) # y position for label
  )

# 3. Add the label to your plot
work.fineness1 <- work.fineness +
  geom_text(
    data = model_stats,
    aes(x = x, y = y, label = label),
    hjust = 0.75, vjust = -5,    # top-left corner
    size = 10
  )

work.fineness1 #no figure to save because of AICc model selection


```


### Compare null condition models to fineness SP work models 
```{r}
library(AICcmodavg)

# ========= body size ========
str(SP_noLLnoGH)
model1 <- lm(Estimated_Fineness ~ 1, data = SP_noLLnoGH)
model2 <- lm(Estimated_Fineness ~ Avg_Work, data = SP_noLLnoGH)
summary(model2)

#table of AIC results
mynames1 <- paste("model", as.character(1:2), sep = "")
models <- list(model1, model2)

# Generate AICc table
myaicc1 <- aictab(models, modnames = mynames1)
print(myaicc1)


```
